{% extends 'model_base.html' %}

{{super()}}

{% block title_model %} 
Support-Vector Machine: Hepatic Disease Prediction
{% endblock %}

{% block subtitle %}
Dataset: Hepatitis C <a href="https://www.kaggle.com/datasets/fedesoriano/hepatitis-c-dataset" target="_blank">[0]</a>
{% endblock %}

{% block model_content %}
<h4>
    The presented work is an example of the SVC library from the Sklearn package, used to perform different Machine Learning techniques, in this case: Support Vector Machine Algorithm.
</h4>
<br>
<p>
    The data classes are multi-labeled, but for the purposes of the model, is going to be encoded to make it a binary classification problem. The data is unbalanced, so F1-Score for evaluation would be used as primary metrics along with the corresponding confusion matrix. The data is highly dimentional, and for the purposes of the analysis a Principal Component Analysis would be fitted for the data.
</p>
<p>
    The language used to perform the analysis and create the model is Python. And the libraries used for the task are:
    <ul>
        <li>Matplotlib and Seaborn to plot the data </li>
        <li>Numpy and Pandas for data manipulation</li>
        <li>Sklearn to train the model and measure the F1-Score</li>
        <li>Mlxtend to plot the boundary limits</li>
    </ul>
</p>
<h5>About the data:</h5>
<p>
    The dataset consists of laboratory measurements with 615 entries and 12 parameters plus a class label. The units of the data are mostly in g/L. The parameter are:
    <ul>
       <li>Category: Nominal value, indicates the status of the patient</li>
       <li>Age: Numerical discrete value</li>
       <li>Sex: Categorical value (m for male and f for female)</li>
       <li>ALB: Albumin (Most important protein in the body; transport and liquid pressure function)</li>
       <li>ALP: Alkaline Phosphatase (Enzime of the bones and liver, increases in inflamatory diseases)</li>
       <li>ALT: Alanine aminotransferase (Functional enzyme of the liver)</li>
       <li>AST: Aspartate aminotransferase: (Functional enzyme of the liver)</li>
       <li>BIL: Bilirrubin (Indicates liver disease associated with the gallbladder)</li>
       <li>CHE: Cholinesterase (Hepatic enzyme)</li>
       <li>CHOL: Cholesterol (Energetic and structure biomolecule free in blood)</li>
       <li>CREA: Creatinine (Mostly a biomarker of renal disease)</li>
       <li>GGT: Gamma-glutamyl transferase (Another enzime that indicates liver function and gallblader diseases)</li>
       <li>PROT: Total proteins in serum</li>   
    </ul>
</p>
<p>More details for this dataset could be found in the <a href="#">EDA Analysis.</a></p>
<br>
<h5>A word on the SVM Algorithm</h5>
<p>
    As explained in the lateral card, the goal of SVM Algorithm is create a boundary to separate classes of objects, this is done using an hyperplane in the equation form of
    $$ \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p = 0 $$
    for p-dimensional data. Since multiple hyperplanes can be generated for the same model, how we choose the best? The solution relies in the maximal margin hyperplane (or optimal separating hyperplane), solution to the next optimization problem:
    $$ \max\limits_{\beta_0,\beta_1,...,\beta_n,M} M $$
    $$ \text{subject to} \sum_{j=1}^p \beta_j^2 = 1 $$
    $$ y_i(\beta_0 + \beta_1 x_i1 + \beta_2 x_i2 + ... + \beta_p x_ip = 0) \geq M \text{  }\forall i = 1,...,n  $$
    the last equation ensuring that every observations is on the correct side of the boundary, and the second makes a minimum distance M to the hyperplane\(\text{.}^1\) The full ensemble for SVM involves kernels (vg. radial, polynomial, etc) to tackle non-linear problems, and optimization problems accord to it. Thankfully to Sklearn the mathematical concerns about creating a model tends to minimize.
</p>
<br>
<h5>The processing of the data follows the next steps:</h5>
<ol>
    <li>Create the dataset with pandas and format the dataframe</li>
    <li>Split in Train and Test set</li>
    <li>Imput the NA values with the mean for each category</li>
    <li>LASSO Model for Variable Selection</li>
    <li>Filter the data with the selected parameters</li>
    <li>Normalize the data and perform PCA to reduce the dimension to two</li>
    <li>Plot the data before training</li>
</ol>
<p>After processing the data, looks like now is linearly separable.</p>
<img class="img-fluid" src="{{url_for('static', filename='plotsvm.png')}}">
<p>The last step is train the SVM model and generate the metrics to assess the generalization of the model</p>
<br>
<h5>Metrics for the SVM classification problem:</h5>
<p>
    In the Test data the Confusion matrix was the next
</p>
<a>

</a>
<p>
    The F1-score  for the xxx category was xxxx, and for the xxxx was xxx. The specificity was xxx, and the sensitivity was xxx, meaning that the model generalizes better the XXX category.
</p>
<br>
<h5>
    Conclusions:
</h5>
<p>
    XXXXX
</p>
<br>
<h5>References</h5>
<p>\(\text{}^1\) BOOKAUTHOR.REFERENCE.XXX</p>
<br>
<h3>LOREM IPSUM</h3>
<br>
<br>
{% endblock %}

{% block card_header %}
What's SVM?
{% endblock %}

{% block titlecard %}
Support-Vector Machine
{% endblock %}

{% block card_content %}
SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data are transformed in such a way that the separator could be drawn as a hyperplane. Following this, characteristics of new data can be used to predict the group to which a new record should belong.<a class="link-info" href="https://www.ibm.com/docs/it/spss-modeler/SaaS?topic=models-how-svm-works" target="_blank">[1]</a>

{% endblock %}

{% block button %}
<a href="/svmdemo" class="btn btn-primary">Demo</a>
{% endblock %}
